## Residual Attention Network


Attention not only serves to select a focused location but also enhances different representations of objects at that location. No attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural network. Inspired by the attention mechanism and recent advances in the deep neural network, authors proposed Residual Attention Network, a convolutional network that adopts mixed attention mechanism in "very deep" structure.

Apart from more discriminative feature representation brought by the attention mechanism, this model also exhibits following appealing properties. 1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. 2) It is able to incorporate with state-of-the art deep network structures in  an end-to-end training fashion. And these properties are made possible with following contributions. 1) Stacked network structure 2) Attention Residual Learning 3) Bottom up top down feedforward attention

![architecture](https://user-images.githubusercontent.com/90513931/215976479-b7ef8e58-df15-4b1d-af5a-2edc4289c349.png)

Their Residual Attention Network is construted by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state of the art network structures. And mask branch can not only serve as a feature selector during forward inference, but also as a gradient update filter durig back propagation. The whole structure can be trained end-to-end. However, naive stacking Attention Modules leads to the obvioius performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit. So they proposed attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. In addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch;s feature selection ability. By using attention residual learning, increasing depth of the propsed Residual Attention Network can improve performance consistently.

![Attention_module](https://user-images.githubusercontent.com/90513931/215976484-10456f8e-db96-4f32-b38e-e4fcc90c643f.png)

#### Sofk Mask Branch

From input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. This mask brach aims at improving trunck branch features rather than solving a complex problem directly. 

THe Attention-56 network outperforms ResNet-152 by a large margin with a 0.4% reduction on top-1 error and a 0.26% reduction on top-5 error. More importantly, Attnetion-56 network achieves better performance with only 52% parameters and 56% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.

![error_rate](https://user-images.githubusercontent.com/90513931/215976485-103801d8-7242-48ca-97aa-65bffb98375f.png)
