## ConvNext


CNNs have several built-in inductive biases that make them well suited to a wide variety of computer vision applications. Themost  important one is translation equivariance, which is a desirable property for tasks like objection detection. In the tear 2020, as the introduction of Vision Transformers completely altered the landscape of network architecture design. Vit introduce no image speicifc inductive bias and makes minimal changes to the original NLP Transformers. One primary focus of Vit is on the scaling behavior: with the help of larger model and dataset sizes, Transformers can outperform standard ResNets by a significant margin. Without the CNNs inductive bias, a vanila Vit model faces many challenges in begin adopted as a gegneric vision backbone. The biggest challenge is ViT's global attention design, which has a quadratic complexity with respect to the input size. This might be acceptable for ImageNet classficiation, but quickly becomes intractable with higher-resolution inputs. The 'sliding window' strategy was reintroduced to Transformers, allowing them to behave more similarily to CNNs. Swin Transformer is a milestone work in this direction, demonstrating for the first time that Transformers can be adopted as a generic vision backbone and achieve state-of-the-art performance across a range of computer vision tasks beyond image classification. Swin Transformer's success and rapid adoption also revealed one thing: the essence of convolution is not becoming irrelevantl rather, it remains much desired and has never faded. 

A naive implementation of sliding window self-attention can be expensive; with advanced approaches such as cycic shifting, the speed can be optimized but the system becomes more sophisticated in design. The only reason CNNs appear to be losing steam is that Transformers surpass them in many vision tasks, and the performance difference is usuallyattributed to the superior scaling behavior of Transformers, with multi head self attention being the key component. In this work, they investigate the architectural distinctions between CNNs and Transformers and try to identify the confounding variables when comparing the network performance. Their exploration is directed by a key question. "How do design decisions in Transforers impact CNNs performance?"

To comparing performances, they set the starting point is a ResNet-50. They then study a series of design decision which they summarized as 1) macro design, 2) ResNext, 3) inverted bottleneck, 4) large kernel size, and 5) various layer wise migcro designs. Apart from the design of network architectures, the training proceduce also affects the ultimate performance. Recent studies demonstrate that a set of modern ntraining techniques can significantly enhance the performance of a simple ResNet-50. By itself, thos enhaced trainign recipe increased the performance of the ResNet-50 model frm 76.1% to 78.8%, implying that a significant portion of the performance difference between tranditional CNNs and ViT may be due to the training techniques.
