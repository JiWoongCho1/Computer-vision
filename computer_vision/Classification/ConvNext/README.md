## ConvNext


CNNs have several built-in inductive biases that make them well suited to a wide variety of computer vision applications. Themost  important one is translation equivariance, which is a desirable property for tasks like objection detection. In the tear 2020, as the introduction of Vision Transformers completely altered the landscape of network architecture design. Vit introduce no image speicifc inductive bias and makes minimal changes to the original NLP Transformers. One primary focus of Vit is on the scaling behavior: with the help of larger model and dataset sizes, Transformers can outperform standard ResNets by a significant margin. Without the CNNs inductive bias, a vanila Vit model faces many challenges in begin adopted as a gegneric vision backbone. The biggest challenge is ViT's global attention design, which has a quadratic complexity with respect to the input size. This might be acceptable for ImageNet classficiation, but quickly becomes intractable with higher-resolution inputs. The 'sliding window' strategy was reintroduced to Transformers, allowing them to behave more similarily to CNNs. Swin Transformer is a milestone work in this direction, demonstrating for the first time that Transformers can be adopted as a generic vision backbone and achieve state-of-the-art performance across a range of computer vision tasks beyond image classification. Swin Transformer's success and rapid adoption also revealed one thing: the essence of convolution is not becoming irrelevantl rather, it remains much desired and has never faded. 

A naive implementation of sliding window self-attention can be expensive; with advanced approaches such as cycic shifting, the speed can be optimized but the system becomes more sophisticated in design. The only reason CNNs appear to be losing steam is that Transformers surpass them in many vision tasks, and the performance difference is usuallyattributed to the superior scaling behavior of Transformers, with multi head self attention being the key component. In this work, they investigate the architectural distinctions between CNNs and Transformers and try to identify the confounding variables when comparing the network performance. Their exploration is directed by a key question. "How do design decisions in Transforers impact CNNs performance?"

To comparing performances, they set the starting point is a ResNet-50. They then study a series of design decision which they summarized as 1) macro design, 2) ResNext, 3) inverted bottleneck, 4) large kernel size, and 5) various layer wise migcro designs. Apart from the design of network architectures, the training proceduce also affects the ultimate performance. Recent studies demonstrate that a set of modern ntraining techniques can significantly enhance the performance of a simple ResNet-50. By itself, thos enhaced trainign recipe increased the performance of the ResNet-50 model frm 76.1% to 78.8%, implying that a significant portion of the performance difference between tranditional CNNs and ViT may be due to the training techniques.

#### Macro Design

The heavy 'res4' stage was meant to be compatiable with downstream tasks like object detection, where a detecor head operates on the 14 x 14 feature plane. Swin-T, on the other hand, followed the same principle but with a slightly different stage compute ratio of 1:1:3:1. For larger Swin Transformers the ratio is 1:1:9:1. Following the design, they adjust the number of the blocks in each stage from (3,4,6,3) in ResNet-50 to (3,3,9,3), which also aligns the FLOPs with Swin-T. This improves the model accuracy from 78.8% to 79.4%. The stem cell in standard ResNet contains a 7x7 convolution layer with stride 2, followed by a max pool, which results in a 4x downsampling of the input images. In vision-Transformers a more aggressive 'patchify' strategy is used as the stem cell, which corresponds to a large kernel size and non overlapping convolution. They replace the ResBet-style stem cell with a patchify layer implemented using a 4x4, stride 4 convolutional layer. The accuracy has chaned from 79.4$ to 79.5%.

#### ResNeXt-ify

ResNeXt model has a better FLOPs/accuracy trade-off than a vanila ResNet. THe core component is grouped convolution, where the convolutional filters are separated into different groups. More precisely, ResNeXt employs grouped convolution for the 3x3 conv layer in a bottleneck block. The combination of depthwise conv and 1x1 convs leads to a separation of spatial and channel mixing, property shared by vision Transformers, where each operation either mixes information across spatial or channel dimension, but not both. The use of depthwise convolution efectively reduces the network FLOPs and, as expected, the accuracy. THis brings the network performance to 80.5% with increased FLOPs.

#### Inverted Bottleneck

This Transformer design is connected to the inverted bottleneck design with an expansion ratio of 4 used in CNNs.  The idea was popularizezd by MobileNetV2, and has subsequently gained traction in several advanced CNNs architectures. They explore the inverted bottleneck design. Despite the increased the FLOPs for the depthwise convolution layer, this change reduces the whole network FLOPs to 4.6G, due to the significant FLOPs reduction in the downsampling residual blocks' shortcut 1x1 convlayer. This results in slightly imporved performance from 80.5% to 80.6%.

#### Large Kernel Sizes

