## ConvNextV2

The performance of a visual representation learning system is largely influenced by three main factors: the neural network architecture chosen, the method used for training the network, and the data used for training. Innovation in neural network architecture design has consistently played a major role in the field of representation learning. Convolutional neural network architectures have had a significant impact on computer vision research by allowing for the use of generic feature learning methods for a variety of visual recognition tasks, rather than relying on manual feature engineering. More recently, ConvNeXt architecture has modernized tranditional CNNs and demonstrated that pure convolutional models could also be scalable architectures. However, the most common method for exploring the design space for neural network architectures is still through benchmarking supervised learning performance on ImageNet.


In a separate line of research, the focus of visual representation learning has been shifting from supervised learning with labels to self-supervesied pre-training with pretext objectives. Among many different self-supervised algorithms, masked autoencoders(MAE) have recently brought success in masked language modeling to the vision domain and quickly become a popular approach for visual representation learning. In fact, previous research has shown that training CNNs with mask-based self-supervised learning can be difficult, and empirical evidence suggests that transformers and CNNs may have different feature learning behaviors that can affect representation quality. So authors propose to _co-design_ the network architecture and the masked autoencoder under the same framework, with the aim of making mask-based self supervised learning effecitve for CNNs models and achieving results similar to those obtained using transformers. In designing the masked autoencoder, they treat the masked input as a set of sparse patches and use sparse convolutions to process only the visible parts. To further improve the pre-training efficiency, they replace the transformer decoder with a single ConvNeXt block, making the entire design fully convolutional. 

![model comparison](https://user-images.githubusercontent.com/90513931/219844372-63ad5a5e-7e59-49c2-a167-9a6f22eefc2b.png)

#### Masked Autoencoders

Masked image modeling, represented by masked autoencoders, is one of the latest self-supervised learning strategies. To the best of their knowledge, there are no pre-trained models that show self-supervised learning can improve upon the best ConvNeXt supervised results.

#### Fully Convolutional Masekd Autoencoder

The learning signals are generated by randomly _masking_ the raw input visuals with a high masking ratio and letting the model predict the missing parts given the remaining context. They use a ranodm masking strategy with a masking ratio of 0.6. As the convolutional model has a hierarchical desige, where the features are downsampled in different stages, the mask is generated in the last stage and upsampled recursively up to the finest resolution. And they use ConvNeXt model as the encoder. One challenge in making masked image modeling effective is preventing the model from learning shortcuts that allow it to copy and paste information from the masked regions. To tackle this issue, their new insight is to view the masked image from a 'sparse data perspective', which was inspired by learning on sparse point clouds in 3D tasks. Their key observation is that the masked image can be represented as a 2D sparse array of pixels. In practice, during pre-training, they propose to convert the standad convolution layer in the encoder with the submanifold sparse convolution, which enables the model to operate only on the visible data points. And they use a lightweight, plain ConvNeXt block as the decoder. This forms an asymmetric encoder-decoder architecture overall, as the encoder is heavier and has a hierarchy. They also considered more complex decodeers such as hierarchical decoders or transformers, but the simpler single ConvNeXt block decoder performed well in terms of fine-tuning accuracy and reduced pre-training time considerably.

![FCMAE framework](https://user-images.githubusercontent.com/90513931/219844369-b2097db7-c64f-4841-b06b-009d86308ed4.png)


They compute the mean squared error(MSE) between the reconstructed and target images. Similar to MAE, the target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches.

They now present a Fully Convolutional Masked AutoEncoder by combinig the proposals described above. Throughout the paper, they focus on the end-to-end fune-tuning performance because of its practical relevance in transfer learning, and use that to assess the quality of the learned representation.

#### Global Response Normalization

They introduce a new glboal Response Normalization technique to make FCMAE pre-training more effective in conjuction with the ConvNeXt architecture. They visualize the activations of a FCMAE pre-trained ConvNeXt-base model and notice an intriguing 'feature collapse' phenomenon: there are many dead or saturated feature maps and the activation becomes redundant across channels. This behavior was mainly observed in the dimension-expansion MLP layers in a ConvNeXt block. The FCMAE pretrained ConvNeXt model exhibits a clear tendency towards feature collapse, consistent with their observations from the previous activation visualizations. 

![feature cosine distance](https://user-images.githubusercontent.com/90513931/219844371-a759dbbb-280d-40d8-bb13-27dd9b569236.png)

This motivates them to consider ways to diversity the features during learning and prevent feature collapse. In this work, they introduce a new response normalization layer called global response normalization(GRN), which aims to increase the contrast and selectivity of channels. The core GRN unit is very ease to implement, requiring only three lines of code, and has no learnable parameters.

![Algorithm of GRN](https://user-images.githubusercontent.com/90513931/219844889-f0435809-8cd3-428f-aa51-7a893848a328.png)


They empirically found that LayerScale becomes unnecessary when GRN is apllied and can be removed. Next figure show that ConvNeXt V2 block. These models range from lightweight to compute intensive ones. 

![ConvNeXtV2 block](https://user-images.githubusercontent.com/90513931/219844364-0247f26d-1339-4d66-b354-f52507d9e40a.png)

Another way to enhance competition across neurons is to use dynamic feature gating methods: squeeze and excite(SE) and convolutional block attention module(CBAM). SE focuses on channel gating, while CBAM focuses on spatial gating. Both modules can increase the contrasts of individual channels, similar to what GRN does. GRN is much simpler and more efficient as it does not require additional parameter layers. 


They present and analyze two key proposals, the FCMAE _pre-training framework_ and ConvNeXt V2 architectures, which are co-designed to make masked-based self-supervised pretrainign successful. Furthrmore, they show that their largest ConvNeXt V2 Huge model, which has been pretrained using the FCMAE framework and fine-tuned on the ImageNet-22K dataset, can achieve a new state of the art of 88.9% top-1 accuracy onthe ImageNet-1K, using only publicly available data. They found that using te FCMAE feamework without modifying the model architecture has a limited impact on rerpesentation learning quality. Similarly, the new GRN layer has a rather small effect on performance under the supervised setup. However, the combination of the two results in a significant improvement in fine-tuning performance. This supports the idea that both the model and learneing framework should be considered together, particularly when it comes to self-supervised learning.

#### object detecion

They finetune Mask-R-CNN on the COCO dataset and report the detetion mAP and the segmentation mAP on the COCO val 2017set. The result is the next figure. Upon this , the model further benefits from better initialization when moving from supervised to FCMAE-based self supervised learning.

![detection comparison](https://user-images.githubusercontent.com/90513931/219844366-5ff52089-24c5-45a1-b4d4-944ef20bfa73.png)
