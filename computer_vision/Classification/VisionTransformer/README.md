## Vision Transformer


Self-attention based architectures, in particular Transformers, have become the model of choice in natural language processing. In computer vision, however, convolutional architectures remain dominat. Therefore, in large-scale image recongition, classic ResNet architecutres are still state of the art. So they experiment with applying a standard Transformer directly to images, with the fewest possible modifications. They find that large scale training trumps inductive bias. Their Vision Transformer attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewe datapoints.

#### Inductive bias

They note that Vision Transformer has much less image specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into eah layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self attention layers are global. The two dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine tuning time for adjusting the position embeddings for images of different resolution. Other than that, the position embeddings at initialization timecarry no information about the 2D positions of the patchs and all spatial relations between the patches have to be learned from scratch.

Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been traied in the past. Most related to theirs is the model of Cordonnier(2020) which extracts patchs of size 2x2 from the input image and applies full self-attention on top. Moreover, they use a small patch size 2x2 pixels, which makes the model applicable only to small resolution images, while, in paper, it can handle medium resolution images as well.


This figuere show the overview of the model. They split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resilting sequence of vectors to a standard Transformer encoder. In order to perform classification, they use the standard approach of adding an extra learnable claasification token to the sequence. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, reshape the image into a sequence of flattened 2D patches. The Transformers uses constant latent vector size D through all of its layers, so they flatten the parches and map to D dimensions with a trainable linear projection. They refer to the output of this projection as the patch embeddings. Similar to BERT's token, they pretend a learnable embeddings to the sequence of embedded patches, whose state at the output of the Transformer encoder serves as the image representation y. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fune tuning time. Position embeddings are added to the patch embeddings to retain positional information. Transformer encoder consists of alternating layers of multihead self attention and MLP block. Layernorm is applied before every block, and residual connections after every block and MLP contains to layers with a GELU non linearity.

They evaluate the representation learning capabilites of ResNet, Vision Transformer and the hybrid. To understand the data requirements of each model, they pre-train on datasets of varying size and evaluate many benchmark tasks. This table show the reslut. The Vision Transformer performs well when pre-trained on a large JFT300M dataset.
